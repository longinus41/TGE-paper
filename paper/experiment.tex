% !TEX root = main.tex

\section{Experiments}
In this section, we conduct experiments to evaluate the performance of our method via node classification on ETG.
fdsfsdfs

\subsection{Data Collection and Graph Construction}
We collect all data by running Ethereum client\footnote{Parity Ethereum Client, https://www.parity.io/ethereum/} which maintains the same copy of blockchain with all historical transactions. Note that we choose the transaction logs on Ethereum from January 1, 2018 to March 31, 2018 (\textcolor{red}{xxxx} external transactions and internal transactions both) as the input of graph construction since it is the most active period with various activities.

By parsing the transactions, 16,599,825 active accounts are obtained, including 14,450,993 EOAs and 2,148,831 SCs. Then we construct the original ETG based on these accounts and transactions.

Specially, we extend the pre-processing scheme to adapt our model. First we construct four relation graphs, which contains ETH transfer graph, contract creation graph, contract invocation graph and mining reward graph. In each graph, repeated edges between the same node pair are merged via the method introduced in section \ref{section:time}.

Last, a test set of accounts with label introduced before is provided to evaluate classification accuracy. It is hard to reveal the identity of addresses since the anonymity of blockchain. We obtain these labeled examples in two ways, \emph{Etherscan}\footnote{Etherscan LabelCloud, https://etherscan.io/labelcloud} and \emph{Searchain}\footnote{Searchain, http://www.searchain.io/}.




\subsection{Experimental Set-Up and Baselines}
As a baseline for our experiments, we compare against state-of-the-art classification results from \textcolor{red}{DeepWalk~\cite{perozzi2014deepwalk}, GCN~\cite{kipf2016semi}, rGCN~\cite{schlichtkrull2018modeling} and ...}.

Unless otherwise noted, all the GCN-based hidden layer has 16 units. Models are trained with Adam optimizer for 100 epochs, and dropout with $dropout\_rate=0.5$ is utilized to avoid overfitting.


\begin{table*}
\footnotesize
\centering
\caption{}
\resizebox{\textwidth}{17mm}{
\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}
\toprule
 & \multicolumn{3}{c|}{Deepwalk} & \multicolumn{3}{c|}{PARW} & \multicolumn{3}{c|}{rGCN} & \multicolumn{3}{c|}{rGCN+asymmetric proximity} & \multicolumn{3}{c}{Ours} \\
\midrule
& \textbf{Precision} & \textbf{Recall} & $\mathbf{F_1}$ & \textbf{Precision} & \textbf{Recall} & $\mathbf{F_1}$ & \textbf{Precision} & \textbf{Recall} & $\mathbf{F_1}$ & \textbf{Precision} & \textbf{Recall} & $\mathbf{F_1}$ & \textbf{Precision} & \textbf{Recall} & $\mathbf{F_1}$ \\
\midrule
 phish and hack & 0.609 & 0.394 & 0.479 &0.565 & 0.333 & 0.419 & 0.913& 0.212& 0.344& 0.720& 0.727& 0.724& 0.714& 0.758& 0.735\\
 token contract & 0.857& 0.735 & 0.791 &0.354& 0.718& 0.475& 0.908& 0.602& 0.724& 0.958& 0.939& 0.949& 0.958& 0.939& 0.949\\
 exchange deposit & 0.586 & 0.531 & 0.557 &0.692& 0.281& 0.400& 0.688& 0.440& 0.537& 0.615& 0.640& 0.628& 0.556& 0.600& 0.579\\
 exchange root & 0.647 & 0.759 & 0.698 &0.667& 0.759& 0.710& 0.923& 0.686& 0.787& 0.862& 0.714& 0.781& 0.828& 0.686& 0.750\\
 pool & 0.692 & 0.750 & 0.720 &0.789& 0.625& 0.697& 1.000& 0.727& 0.842& 0.842& 0.727& 0.781& 1.000& 0.727& 0.842\\
 miner & 0.400 & 0.694 & 0.508 &0.667& 0.872& 0.756& 0.867& 0.951& 0.907& 0.826& 0.927& 0.874& 0.841& 0.902& 0.871\\
 primary market & 0.405 & 0.548 & 0.466 & 0.727& 0.516& 0.604& 0.739& 0.548& 0.630& 0.680& 0.548& 0.607& 0.750& 0.484& 0.588\\
 ICO wallet & 0.364 & 0.353 & 0.358 & 0.630& 0.500& 0.558& 0.546& 0.158& 0.245& 0.769& 0.526& 0.625& 0.742& 0.605& 0.668\\
 \midrule
 average & 0.614 & 0.583 & 0.585 & 0.623& 0.577& 0.570& \bf{0.848}& 0.496& 0.593& 0.806& 0.761& 0.779& 0.811& \bf{0.764}& \bf{0.782}\\
\bottomrule
\end{tabular}
}
\end{table*}

All embedding and classification programs were run on the server, which includes Intel Xeon E5 CPU with 55 processors and 128GB of memory, and the GPU used for deep learning is Nvidia 1080Ti.

\subsection{Results}
We use three indicators to evaluate each model, precision, recall and $F_1$ score. The $F_1$ score of label $i$ is computed as $F_1^i=2\frac{Precison^i \times Recall^i}{Precison^i + Recall^i}$, and the average $F_1$ score is computed as $F_1^{avg}=2\frac{Precison^{all} \times Recall^{all}}{Precison^{all} + Recall^{all}}$, where $Precison^{all}$ and $Recall^{all}$ are the precision and recall value of all labels.

%These data are collected

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "analysis"
%%% End:
